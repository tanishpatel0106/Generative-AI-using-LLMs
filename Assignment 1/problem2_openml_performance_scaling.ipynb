{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Problem 2 - OpenML, Algorithmic Performance Scaling (25 points)\n",
                "\n",
                "This notebook explores classification tasks using datasets from OpenML, comparing Random Forest and Gradient Boosting classifiers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Setup: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "import warnings\n",
                "from sklearn.datasets import fetch_openml\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "\n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Load and Summarize Datasets [5 points]\n",
                "\n",
                "**Task:** Select 2 datasets from OpenML with different number of output classes and summarize their attributes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset 1: Choose a dataset with multi-class classification\n",
                "# Dataset 2: Choose a dataset with binary classification\n",
                "\n",
                "print(\"Loading datasets from OpenML...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# TODO: Load Dataset 1\n",
                "# dataset1 = fetch_openml(...)\n",
                "\n",
                "# TODO: Load Dataset 2\n",
                "# dataset2 = fetch_openml(...)\n",
                "\n",
                "print(\"Datasets loading section complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def summarize_dataset(data, target, name):\n",
                "    \"\"\"Summarize attributes of a dataset.\"\"\"\n",
                "    # TODO: Implement dataset summarization\n",
                "    # 1. Count features\n",
                "    # 2. Count instances\n",
                "    # 3. Count classes\n",
                "    # 4. Count numerical vs categorical features\n",
                "    \n",
                "    summary = {\n",
                "        'Dataset': name,\n",
                "        # 'Number of Features': ...,\n",
                "        # ...\n",
                "    }\n",
                "    \n",
                "    return summary, []\n",
                "\n",
                "# TODO: Call summarize_dataset for both datasets and display results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Training and Evaluation [15 points]\n",
                "\n",
                "**Task:** \n",
                "- Split 80% training / 20% test\n",
                "- Generate 10 subsets by randomly subsampling 10%, 20%, ..., 100% of training set\n",
                "- Train Random Forest and Gradient Boosting classifiers\n",
                "- Measure training time and test accuracy\n",
                "- Generate learning curves and training time curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_data(data, target):\n",
                "    \"\"\"Prepare data for training - handle categorical variables and encode labels.\"\"\"\n",
                "    # TODO: Handle encoding of categorical features and target labels\n",
                "    return data, target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(X, y, dataset_name, random_state=42):\n",
                "    \"\"\"\n",
                "    Run the training experiment with 10 different training set sizes.\n",
                "    \n",
                "    Returns:\n",
                "        results: dict containing accuracies and training times for both classifiers\n",
                "    \"\"\"\n",
                "    # TODO: Split data into 80% training and 20% test\n",
                "    \n",
                "    # TODO: Initialize results dictionary to store metrics\n",
                "    # results = { ... }\n",
                "    \n",
                "    # Training percentages\n",
                "    percentages = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
                "    \n",
                "    print(f\"\\n{'='*70}\")\n",
                "    print(f\"Dataset: {dataset_name}\")\n",
                "    print(f\"{'='*70}\")\n",
                "    \n",
                "    for pct in percentages:\n",
                "        # TODO: Create training subset (handle sampling)\n",
                "        \n",
                "        # TODO: Train Random Forest and measure time/accuracy\n",
                "        \n",
                "        # TODO: Train Gradient Boosting and measure time/accuracy\n",
                "        \n",
                "        # TODO: Store results\n",
                "        pass\n",
                "    \n",
                "    return {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_results(results, dataset_name):\n",
                "    \"\"\"\n",
                "    Generate learning curves and training time curves.\n",
                "    \"\"\"\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    # TODO: Extract data from results dictionary\n",
                "    \n",
                "    # TODO: Plot Learning Curves (Accuracy vs Data Size) on axes[0]\n",
                "    \n",
                "    # TODO: Plot Training Time Curves (Time vs Data Size) on axes[1]\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Run Experiment and Plot for Dataset 1\n",
                "# X1, y1 = prepare_data(...)\n",
                "# results1 = run_experiment(X1, y1, 'Dataset 1')\n",
                "# plot_results(results1, 'Dataset 1')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Run Experiment and Plot for Dataset 2\n",
                "# X2, y2 = prepare_data(...)\n",
                "# results2 = run_experiment(X2, y2, 'Dataset 2')\n",
                "# plot_results(results2, 'Dataset 2')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Analysis and Observations [5 points]\n",
                "\n",
                "**Task:** Write three main observations about:\n",
                "1. Scaling of training time\n",
                "2. Comparison of accuracy between the two classifiers\n",
                "3. Learning curve behavior"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Your Observations:\n",
                "\n",
                "**Observation 1: Training Time Scaling**\n",
                "- [Your answer here]\n",
                "\n",
                "**Observation 2: Accuracy Comparison**\n",
                "- [Your answer here]\n",
                "\n",
                "**Observation 3: Learning Curve Behavior**\n",
                "- [Your answer here]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}