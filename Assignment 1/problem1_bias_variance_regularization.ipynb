{"cells":[{"cell_type":"markdown","metadata":{"id":"x06lVeUV8UUF"},"source":["# Problem 1: Bias-Variance Tradeoff and Regularization\n","\n","In this problem, we will explore the fundamental concepts of the bias-variance tradeoff and demonstrate how regularization affects model performance."]},{"cell_type":"markdown","metadata":{"id":"nKDAIxUr8UUG"},"source":["---\n","## Part 1: Deriving the Bias-Variance Decomposition [15 points]\n","\n","**Task:** Show mathematically that the expected test error can be decomposed as:\n","\n","$$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n","\n","where:\n","- $y = f(x) + \\epsilon$ is the true target with noise $\\epsilon \\sim N(0, \\sigma^2)$\n","- $\\hat{f}(x)$ is our estimated function\n","- $\\text{Bias} = E[\\hat{f}(x)] - f(x)$\n","- $\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$"]},{"cell_type":"markdown","metadata":{"id":"uHsuhfRm8UUG"},"source":["### Derivation\n","\n","*Write your derivation here:*\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ErnTte4x8UUG"},"source":["---\n","## Part 2: Dataset Creation and Visualization [10 points]\n","\n","**Task:** Create a dataset using the following specifications:\n","- True function: $f(x) = x + \\sin(1.5x)$\n","- Add Gaussian noise with variance $\\sigma^2 = 0.3$\n","- Generate 50 training samples with $x \\in [-3, 3]$\n","\n","Then create a visualization showing:\n","1. The noisy training data points\n","2. The true underlying function $f(x)$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DplyoGJl8UUH"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","np.random.seed(42)\n","\n","# TODO: Define the true function f(x)\n","def f(x):\n","    # Your code here\n","    pass\n","\n","# TODO: Define function y(x) that adds noise to f(x)\n","def y(x, noise_var):\n","    # Your code here\n","    pass\n","\n","# TODO: Set parameters\n","n_samples = None  # number of training samples\n","x_range = None    # tuple (min, max) for x values\n","noise_var = None  # noise variance σ²\n","\n","# TODO: Generate training data\n","X_train = None\n","y_train = None\n","\n","# TODO: Create visualization\n","# Your plotting code here\n"]},{"cell_type":"markdown","metadata":{"id":"EfIwVvpy8UUH"},"source":["---\n","## Part 3: Polynomial Fitting - Underfitting vs Overfitting [15 points]\n","\n","**Task:** Demonstrate underfitting and overfitting using polynomial regression:\n","1. Fit a **low-degree polynomial** (e.g., degree 1) to show underfitting\n","2. Fit a **high-degree polynomial** (e.g., degree 15) to show overfitting\n","3. Visualize both fits along with the true function and training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hfwc6RXE8UUH"},"outputs":[],"source":["# TODO: Fit polynomials of different degrees\n","low_degree = 1   # underfitting\n","high_degree = 15  # overfitting\n","\n","# TODO: Fit the polynomials using np.polyfit\n","# Your code here\n","\n","# TODO: Create visualization comparing underfitting vs overfitting\n","# Your plotting code here\n"]},{"cell_type":"markdown","metadata":{"id":"ef6B3Gq58UUH"},"source":["---\n","## Part 4: Bias-Variance Tradeoff Analysis [15 points]\n","\n","**Task:** Empirically demonstrate the bias-variance tradeoff:\n","1. For polynomial degrees 1 to 15, fit models on multiple different datasets (at least 100 datasets)\n","2. Compute the Bias², Variance, and MSE for each degree\n","3. Create a plot showing how these quantities change with model complexity\n","4. Identify the optimal model complexity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOI64jde8UUH"},"outputs":[],"source":["# TODO: Set up parameters for analysis\n","n_datasets = 100  # number of different datasets to generate\n","max_degree = 15   # maximum polynomial degree to test\n","n_test = 200      # number of test points\n","\n","# TODO: Generate fixed test points\n","# Your code here\n","\n","# TODO: For each dataset, fit polynomials of each degree and store predictions\n","# Your code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZcsJ1FO8UUI"},"outputs":[],"source":["# TODO: Compute Bias², Variance, and MSE for each polynomial degree\n","# Your code here\n","\n","# TODO: Print results in a table format\n","# Your code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OH7SjxWq8UUI"},"outputs":[],"source":["# TODO: Create the bias-variance tradeoff plot\n","# Show: Bias², Variance, MSE, Noise floor, and Bias²+Variance+Noise\n","# Mark the optimal model (minimum MSE)\n","# Your plotting code here\n"]},{"cell_type":"markdown","metadata":{"id":"kgrl19Xh8UUI"},"source":["### Part 4: Analysis - Best Model Identification\n","\n","**Can you identify the best model?**\n","\n","*Your answer here:*"]},{"cell_type":"markdown","metadata":{"id":"d72XAic88UUI"},"source":["---\n","## Part 5: L2 Regularization (Ridge Regression) [10 points]\n","\n","**Task:** Apply L2 regularization to the degree-10 polynomial and compare with the unregularized version."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTzb7VMq8UUI"},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# TODO: Set parameters\n","degree = 10\n","alpha = 1.0  # Regularization strength (lambda)\n","\n","# TODO: Fit unregularized and regularized polynomials on multiple datasets\n","# Your code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFxK1wxN8UUI"},"outputs":[],"source":["# TODO: Compute Bias², Variance, and MSE for both models\n","# Your code here\n","\n","# TODO: Display comparison table\n","# Your code here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHiM4De98UUI"},"outputs":[],"source":["# TODO: Create visualization comparing regularized vs unregularized\n","# Show: bar plot comparing metrics and sample predictions\n","# Your plotting code here\n"]},{"cell_type":"markdown","metadata":{"id":"Odb5VkPa8UUI"},"source":["### Part 5: Analysis - Regularization Effect\n","\n","**Does the regularized model have a higher or lower bias?**\n","\n","*Your answer here:*\n","\n","**What about MSE?**\n","\n","*Your answer here:*\n","\n","**Explain:**\n","\n","*Your answer here:*"]},{"cell_type":"markdown","metadata":{"id":"Rc9Hhq2K8UUI"},"source":["---\n","## Summary\n","\n","In this problem, you will:\n","1. Derive the bias-variance decomposition: $E[MSE] = \\text{Bias}^2 + \\text{Variance} + \\text{Noise}$\n","2. Visualize a noisy dataset and the true underlying function\n","3. Demonstrate underfitting (low-degree) and overfitting (high-degree) with polynomial regression\n","4. Quantify the bias-variance tradeoff across model complexities\n","5. Show how L2 regularization trades bias for variance to improve generalization"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}