{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Problem 3: Multilingual Retrieval Augmented Generation (25 points)\n\nImplement a multilingual search and retrieval augmented generation system using the OPUS Books dataset, which contains parallel text in English and Italian. You will create a system that can search across languages and generate content based on the retrieved passages."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Problem 3(a): Setting up the vector search system (8 points)\n\n- Use sentence-transformers' multilingual model `paraphrase-multilingual-MiniLM-L12-v2`\n- Create vector embeddings for the OPUS Books text passages\n- Build a FAISS index for efficient similarity search\n- Save and load the index for reuse"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# \u2500\u2500 Required packages (Python 3.12.6, tested versions) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Pinned install (recommended for reproducibility):\n# !pip install \\\n#     torch==2.10.0 \\\n#     numpy==2.4.2 \\\n#     faiss-cpu==1.13.2 \\\n#     sentence-transformers==5.2.3 \\\n#     transformers==5.2.0 \\\n#     datasets==4.5.0 \\\n#     tqdm==4.67.2 \\\n#     sentencepiece==0.2.1 \\\n#     accelerate==1.12.0\n#\n# Latest versions (may need minor adjustments):\n# !pip install torch numpy faiss-cpu sentence-transformers transformers datasets tqdm sentencepiece accelerate\n\nimport os\n\n# macOS: prevent Metal (MPS) memory conflicts when loading multiple large models\nos.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n\nimport numpy as np\nimport torch\nimport faiss\nimport time\nimport json\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import MBartForConditionalGeneration, MBart50Tokenizer",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Loading and Processing the Dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load 1000 parallel English-Italian text pairs from OPUS Books\ndataset = load_dataset(\"opus_books\", \"en-it\", split=\"train[:1000]\")\n\ntexts_en = [item['translation']['en'] for item in dataset]\ntexts_it = [item['translation']['it'] for item in dataset]\n\nprint(f\"Loaded {len(texts_en)} English passages and {len(texts_it)} Italian passages.\")\nprint(f\"\\nExample English : {texts_en[0][:100]}...\")\nprint(f\"Example Italian : {texts_it[0][:100]}...\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Initialization"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Initialize the multilingual sentence transformer for creating embeddings\n# Model name: 'paraphrase-multilingual-MiniLM-L12-v2'  (hint: pass device='cpu')\nmodel = None  # YOUR CODE HERE\n\n# Initialize the mBART model and tokenizer for multilingual text generation\n# Model name: 'facebook/mbart-large-50-many-to-many-mmt'\n# Note: use MBart50Tokenizer (not AutoTokenizer); pass torch_dtype=torch.float32, device_map=\"cpu\" to the model\ngenerator_model = None       # YOUR CODE HERE\ngenerator_tokenizer = None   # YOUR CODE HERE",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Create Embeddings"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Generate embeddings for all English and Italian passages, then combine them.\n# The final array should have shape (2000, embedding_dim):\n#   - rows 0..999   : English embeddings\n#   - rows 1000..1999: Italian embeddings\n\nembeddings_en = None  # YOUR CODE HERE: encode texts_en\nembeddings_it = None  # YOUR CODE HERE: encode texts_it\nembeddings    = None  # YOUR CODE HERE: stack into one array (hint: np.vstack)\n\nprint(f\"Embedding shape: {embeddings.shape}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### FAISS Indexing for Efficient Similarity Search"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n    \"\"\"\n    Build a FAISS flat L2 index from the given embedding matrix.\n\n    Parameters\n    ----------\n    embeddings : np.ndarray, shape (n, d) -- should be float32\n\n    Returns\n    -------\n    faiss.IndexFlatL2\n    \"\"\"\n    # YOUR CODE HERE\n    # 1. Create a faiss.IndexFlatL2 with dimension d = embeddings.shape[1]\n    # 2. Add the embeddings to the index (ensure dtype is float32)\n    pass\n\n\nindex = build_faiss_index(embeddings)\nprint(f\"Index contains {index.ntotal} vectors.\")\n\n# Save the index to disk and reload it\n# YOUR CODE HERE\n# faiss.write_index(index, \"multilingual_index.faiss\")\n# index = faiss.read_index(\"multilingual_index.faiss\")\n# print(\"Index saved and reloaded successfully.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Problem 3(b): Implement Multilingual Search (8 points)\n\n- Create a search function that accepts queries in either English or Italian\n- Add metadata filtering capability to search in specific languages\n- Return top-k most relevant passages with scores\n- Implement efficient batch processing for multiple queries"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Implementing Multilingual Search"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def search(query: str, lang: str = None, k: int = 5) -> list:\n    \"\"\"\n    Perform semantic search over the multilingual FAISS index.\n\n    Parameters\n    ----------\n    query : str       -- query text in English or Italian\n    lang  : str|None  -- restrict results to 'en', 'it', or None for both\n    k     : int       -- number of results to return\n\n    Returns\n    -------\n    list[dict] -- each dict has keys 'text' (str), 'lang' (str), 'score' (float)\n    \"\"\"\n    # YOUR CODE HERE\n    # 1. Encode the query: query_vector = model.encode([query])\n    # 2. Search the index: D, I = index.search(query_vector, k)\n    # 3. For each (distance d, index i) pair:\n    #      if i < len(texts_en): text = texts_en[i],              lang_code = 'en'\n    #      else:                  text = texts_it[i - len(texts_en)], lang_code = 'it'\n    # 4. If lang is not None, keep only results where lang_code == lang\n    # 5. Return list of dicts with 'text', 'lang', 'score'\n    pass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Testing Multilingual Search"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test queries in both languages\nqueries = {\n    \"English\": \"stories about adventure and discovery\",\n    \"Italian\": \"storie di avventura e scoperta\"\n}\n\nfor lang_name, query_text in queries.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"{lang_name} query: '{query_text}'\")\n    print('='*60)\n\n    start = time.time()\n    results = search(query_text, k=3)\n    elapsed = time.time() - start\n\n    print(f\"Search time: {elapsed*1000:.2f} ms  |  {len(results)} result(s)\")\n    for i, r in enumerate(results, 1):\n        print(f\"  [{i}] ({r['lang'].upper()}, score={r['score']:.4f})  {r['text'][:90]}...\")\n\n# Cross-lingual: English query filtered to Italian results only\nprint(f\"\\n{'='*60}\")\nprint(\"English query -> Italian results only\")\nprint('='*60)\nresults_it = search(\"stories about adventure and discovery\", lang='it', k=3)\nfor i, r in enumerate(results_it, 1):\n    print(f\"  [{i}] ({r['lang'].upper()}, score={r['score']:.4f})  {r['text'][:90]}...\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Problem 3(c): Adding Retrieval-Augmented Generation (RAG) Capabilities (9 points)\n\nIn this section you will add RAG functionality by combining the search system above with\nthe mBART-large-50 language model.\n\n### Tasks\n\n1. **Content generation** -- implement `generate_content(prompt, context)` that feeds a\n   retrieved passage plus an instruction prompt into the mBART model and returns the\n   generated text.\n\n2. **Single-document RAG** -- implement `rag_single(query, prompt)` that retrieves the\n   single most relevant passage and generates content based on it.\n\n3. **Multi-document RAG** -- implement `rag_group(query, prompt, k)` that retrieves the\n   top-k passages, concatenates them as context, and generates a comparative response.\n\n4. **Prompt strategies** -- experiment with different prompt types:\n   - *Recommendation prompt*: `\"Write a short book recommendation based on this excerpt:\"`\n   - *Comparative prompt*: `\"Compare and contrast these excerpts, discussing themes and style:\"`\n\n5. **Testing** -- run the functions with:\n   - English query: `\"stories about adventure and discovery\"`\n   - Italian query:  `\"storie di avventura e scoperta\"`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Implementing RAG Capabilities"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_content(prompt: str, context: str) -> str:\n    \"\"\"\n    Generate text using the mBART model conditioned on a prompt and context.\n\n    Parameters\n    ----------\n    prompt  : str -- instruction for the model\n    context : str -- retrieved passage(s) to base the generation on\n\n    Returns\n    -------\n    str -- generated text\n    \"\"\"\n    # YOUR CODE HERE\n    # 1. Build the input string: input_text = f\"{prompt}\\n{context}\"\n    # 2. Tokenize: inputs = generator_tokenizer(input_text, return_tensors=\"pt\",\n    #              max_length=512, truncation=True)\n    # 3. Generate: outputs = generator_model.generate(\n    #              **inputs,\n    #              forced_bos_token_id=generator_tokenizer.lang_code_to_id[\"en_XX\"])\n    # 4. Decode and return: generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    pass\n\n\ndef rag_single(query: str, prompt: str) -> str:\n    \"\"\"\n    Retrieve the single most relevant passage and generate content based on it.\n\n    Parameters\n    ----------\n    query  : str -- search query\n    prompt : str -- generation instruction\n\n    Returns\n    -------\n    str -- generated text\n    \"\"\"\n    # YOUR CODE HERE\n    # 1. Call search(query, k=1) to get the top result\n    # 2. Extract the 'text' field from the result\n    # 3. Call generate_content(prompt, context) and return the output\n    pass\n\n\ndef rag_group(query: str, prompt: str, k: int = 3) -> str:\n    \"\"\"\n    Retrieve the top-k passages and generate content based on all of them.\n\n    Parameters\n    ----------\n    query  : str -- search query\n    prompt : str -- generation instruction\n    k      : int -- number of passages to retrieve\n\n    Returns\n    -------\n    str -- generated text\n    \"\"\"\n    # YOUR CODE HERE\n    # 1. Call search(query, k=k) to get the top-k results\n    # 2. Join retrieved texts with \"\\n---\\n\" as a separator\n    # 3. Call generate_content(prompt, context) and return the output\n    pass",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Testing RAG Capabilities"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Single-document RAG\nprint(\"=\" * 60)\nprint(\"Single-Document RAG\")\nprint(\"=\" * 60)\nresult = rag_single(\n    query=\"stories about adventure and discovery\",\n    prompt=\"Write a short book recommendation based on this excerpt:\"\n)\nprint(result)\n\n# Multi-document RAG (English query)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Multi-Document RAG  (English query)\")\nprint(\"=\" * 60)\nresult = rag_group(\n    query=\"stories about adventure and discovery\",\n    prompt=\"Compare and contrast these book excerpts, discussing their themes and style:\",\n    k=3\n)\nprint(result)\n\n# Multi-document RAG (Italian query)\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Multi-Document RAG  (Italian query)\")\nprint(\"=\" * 60)\nresult = rag_group(\n    query=\"storie di avventura e scoperta\",\n    prompt=\"Confronta e analizza questi estratti di libri, discutendo i temi e lo stile:\",\n    k=3\n)\nprint(result)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bonus (+5 points): Semantic Caching\n\nImplement a `SemanticCache` class to avoid redundant searches for repeated or highly\nsimilar queries.\n\n- Store each query's embedding alongside its results.\n- For a new query, compute cosine similarity against all cached embeddings.\n- If the maximum similarity exceeds a threshold (e.g. 0.95), return the cached results.\n- Otherwise run a fresh search and add the result to the cache.\n\nMeasure and report the cache hit rate and average query latency with and without caching."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class SemanticCache:\n    def __init__(self, threshold: float = 0.95):\n        \"\"\"\n        Parameters\n        ----------\n        threshold : float -- cosine similarity above which a cache hit is declared\n        \"\"\"\n        self.threshold = threshold\n        self.cache = []  # list of (query_embedding np.ndarray, results list)\n\n    def search_with_cache(self, query: str, lang: str = None, k: int = 5) -> list:\n        \"\"\"\n        Return cached results for semantically similar past queries;\n        otherwise run a fresh search and cache the result.\n\n        Parameters\n        ----------\n        query : str\n        lang  : str|None\n        k     : int\n\n        Returns\n        -------\n        list[dict] -- same format as search()\n        \"\"\"\n        # YOUR CODE HERE\n        # 1. Encode the query with model.encode([query])\n        # 2. For each (cached_emb, cached_results) in self.cache:\n        #      compute cosine similarity between the new embedding and cached_emb\n        #      if similarity >= self.threshold: return cached_results  (cache hit)\n        # 3. Cache miss: call search(query, lang, k), append to self.cache, return results\n        pass",
   "outputs": [],
   "execution_count": null
  }
 ]
}