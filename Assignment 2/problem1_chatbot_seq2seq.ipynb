{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqjmxujRDb-Q"
      },
      "source": [
        "# Problem 1: Training a Simple Chatbot using a Seq-to-Seq Model (25 points)\n",
        "\n",
        "You will train a simple chatbot using movie scripts from the Cornell Movie Dialogs Corpus based on the [PyTorch Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html).\n",
        "\n",
        "This tutorial allows you to train a recurrent sequence-to-sequence model. You will learn the following concepts:\n",
        "\n",
        "- Handle loading and pre-processing of [the Cornell Movie-Dialogs Corpus dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
        "- Implement a sequence-to-sequence model with [Luong attention mechanism(s)](https://arxiv.org/abs/1508.04025)\n",
        "- Jointly train encoder and decoder models using mini-batches\n",
        "- Implement greedy-search decoding module\n",
        "- Interact with the trained chatbot\n",
        "\n",
        "---\n",
        "\n",
        "## Scoring Breakdown\n",
        "\n",
        "| Task | Points |\n",
        "|------|--------|\n",
        "| Task 1: Run the tutorial end-to-end in Colab | 5 |\n",
        "| Task 3: Create W&B sweep configuration | 5 |\n",
        "| Task 4: Run hyperparameter sweeps on GPU Colab | 5 |\n",
        "| Task 5: Analysis of best hyperparameters & feature importance | 10 |\n",
        "| **Total** | **25** |\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "- [The Cornell Movie Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
        "- [Hyperparameter sweeps with Weights and Biases (video tutorial)](https://www.youtube.com/watch?v=9zrmUIlScdY)\n",
        "- [Sample Google Colab project for W&B sweeps](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb)\n",
        "- [Weights and Biases Website](https://wandb.ai/site)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSOYIfhfDb-Q"
      },
      "source": [
        "---\n",
        "## Task 1 [5 points]: Run the Tutorial End-to-End\n",
        "\n",
        "Make a copy of the [PyTorch Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html) notebook, follow the instructions to train and evaluate the chatbot model in your **Google Colab** environment (GPU recommended).\n",
        "\n",
        "The tutorial code is provided below as your starting point. Run each cell in order and verify that the model trains successfully and you can interact with the chatbot at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfYo1VLJDb-R"
      },
      "source": [
        "### Setup: Install Dependencies and Download Data\n",
        "\n",
        "The Cornell Movie-Dialogs Corpus must be downloaded before running the tutorial. The dataset is available via [ConvoKit](https://convokit.cornell.edu/documentation/movie.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (torch is pre-installed in Colab; run the %%writefile cell above first)\n",
        "!pip install \"torch>=2.4.0\" -q\n",
        "!pip install \"convokit>=3.0,<4.0\" -q\n",
        "!pip install \"wandb>=0.18\" -q\n",
        "\n",
        "# Download the Cornell Movie-Dialogs Corpus via ConvoKit\n",
        "import convokit\n",
        "corpus = convokit.Corpus(filename=convokit.download(\"movie-corpus\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GKFiSiRDb-R",
        "outputId": "0de2c40a-da36-4275-8086-632af9d85394"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/219.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "model_directory: ~/.convokit/saved-models\n",
            "default_backend: mem\n",
            "Downloading movie-corpus to /root/.convokit/saved-corpora/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfwaX4tpDb-R"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once in Colab)\n",
        "# Requires PyTorch >= 2.4.0 (pre-installed in Colab; verify with: import torch; print(torch.__version__))\n",
        "!pip install \"convokit>=3.0,<4.0\" -q\n",
        "!pip install \"wandb>=0.18\" -q\n",
        "\n",
        "# Download the Cornell Movie-Dialogs Corpus via ConvoKit\n",
        "import convokit\n",
        "corpus = convokit.Corpus(filename=convokit.download(\"movie-corpus\"))"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVZb9VIXDb-R"
      },
      "source": [
        "### Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsQrLuTJDb-R",
        "outputId": "ff520e4f-cf46-4010-83ea-e07aa5bda2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "assert torch.__version__ >= \"2.4\", f\"PyTorch >= 2.4 required, got {torch.__version__}\"\n",
        "\n",
        "if torch.accelerator.is_available():\n",
        "    device = torch.accelerator.current_accelerator().type\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKm3vsWiDb-R"
      },
      "source": [
        "### Load & Preprocess Data\n",
        "\n",
        "The Cornell Movie-Dialogs Corpus is stored in `utterances.jsonl` format. We parse the raw file to extract consecutive question-answer sentence pairs from each conversation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MzaDK19Db-R"
      },
      "outputs": [],
      "source": [
        "def loadLinesAndConversations(fileName):\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "\n",
        "    return lines, conversations\n",
        "\n",
        "\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxkdVpVyDb-R"
      },
      "source": [
        "#### Create Formatted Data File\n",
        "\n",
        "Parse the corpus and write tab-separated input/output pairs to a text file for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Dtl5FaDb-R",
        "outputId": "ee40fdfc-73ea-4c5c-d401-342878c92cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "b'They do to!\\tThey do not!\\n'\n",
            "b'She okay?\\tI hope so.\\n'\n",
            "b\"Wow\\tLet's go.\\n\"\n"
          ]
        }
      ],
      "source": [
        "# Define paths — update corpus_path to where ConvoKit downloaded the dataset\n",
        "corpus_name = \"movie-corpus\"\n",
        "corpus_path = os.path.join(\"/root/.convokit/saved-corpora\", corpus_name)\n",
        "datafile = os.path.join(corpus_path, \"formatted_movie_lines.txt\")\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "print(\"\\nProcessing corpus...\")\n",
        "lines, conversations = loadLinesAndConversations(\n",
        "    os.path.join(corpus_path, \"utterances.jsonl\")\n",
        ")\n",
        "\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "print(\"\\nSample lines from file:\")\n",
        "with open(datafile, 'rb') as f:\n",
        "    lines_sample = f.readlines()\n",
        "for line in lines_sample[:3]:\n",
        "    print(line)"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjdoo3CDb-S"
      },
      "source": [
        "#### Vocabulary Class\n",
        "\n",
        "The `Voc` class maintains word-to-index and index-to-word mappings. Three special tokens are reserved:\n",
        "- `PAD` (0): padding token used to equalize batch sequence lengths\n",
        "- `SOS` (1): start-of-sequence token fed as the first decoder input\n",
        "- `EOS` (2): end-of-sequence token appended to every target sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "silxvf8mDb-S"
      },
      "outputs": [],
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "        keep_words = []\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITATIVDTDb-S"
      },
      "source": [
        "#### Text Normalization & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFuKep3VDb-S",
        "outputId": "22ea2bd8-1078-4b39-cf01-f7a6c2b5a4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 64313 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 18082\n",
            "\n",
            "Sample pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH = 10  # Maximum sentence length (in words) to consider\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([\\[.!?\\]])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "voc, pairs = loadPrepareData(corpus_name, corpus_name, datafile, save_dir)\n",
        "print(\"\\nSample pairs:\")\n",
        "for pair in pairs[:5]:\n",
        "    print(pair)"
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Ej-EEkDb-S"
      },
      "source": [
        "#### Trim Rare Words\n",
        "\n",
        "Remove words appearing fewer than `MIN_COUNT` times to reduce vocabulary size and improve generalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKtHoHPhDb-S",
        "outputId": "1015f40f-1eca-4c50-9ecb-f469f723b83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 7833 / 18079 = 0.4333\n",
            "Trimmed from 64313 pairs to 53131, 0.8261 of total\n"
          ]
        }
      ],
      "source": [
        "MIN_COUNT = 3\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    voc.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
        "        len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
        "    ))\n",
        "    return keep_pairs\n",
        "\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgB6IWzvDb-S"
      },
      "source": [
        "### Prepare Data for Models\n",
        "\n",
        "Convert sentence pairs into padded tensors suitable for batch training. Sequences in a batch are padded to the same length, and a binary mask is created so that the loss function ignores padding positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWwRhsDeDb-S",
        "outputId": "ed5ef91d-c5d6-4344-af04-286c99c444f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[   7, 5319,   11,  547, 4607],\n",
            "        [ 161,   92,   44,    6,   14],\n",
            "        [  19, 1162,    5,  547,    2],\n",
            "        [   3,   72,   18,    6,    0],\n",
            "        [ 713,   25,   14,    2,    0],\n",
            "        [  72,   36,    2,    0,    0],\n",
            "        [  14,   14,    0,    0,    0],\n",
            "        [  14,    2,    0,    0,    0],\n",
            "        [  14,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([10,  8,  6,  5,  3])\n",
            "target_variable: tensor([[6567,   19,  109,  829,   24],\n",
            "        [   7,   17,   24,   85,   64],\n",
            "        [ 490,   79,  898,    6,  511],\n",
            "        [ 109,  917,   72,    2,  321],\n",
            "        [  24,  578,   34,    0, 1413],\n",
            "        [  10,   10,   10,    0,   14],\n",
            "        [   2,    2,    2,    0,    2]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True],\n",
            "        [ True,  True,  True, False,  True]])\n",
            "max_target_len: 7\n"
          ]
        }
      ],
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    \"\"\"Returns padded input sequence tensor and lengths.\"\"\"\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(l, voc):\n",
        "    \"\"\"Returns padded target sequence tensor, mask, and max target length.\"\"\"\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    \"\"\"Returns all items for a given batch of pairs.\"\"\"\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "# Sanity check\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98JnvOefDb-S"
      },
      "source": [
        "### Define Models\n",
        "\n",
        "#### Encoder\n",
        "\n",
        "The encoder is a bidirectional GRU. For each input token it produces a hidden state; the forward and backward outputs are **summed** to form a single context vector per time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcc9ZAgDb-S"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        # TODO: Define a bidirectional GRU with the given hidden_size, n_layers, and dropout.\n",
        "        # Use nn.GRU — remember to set bidirectional=True.\n",
        "        emb_dim = embedding.embedding_dim\n",
        "\n",
        "        # NOTE: In PyTorch GRU, dropout is only applied if n_layers > 1\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=(0 if n_layers == 1 else dropout),\n",
        "            bidirectional=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # TODO: Embed input_seq, pack the padded sequence, run through the GRU,\n",
        "        # unpack, then SUM the forward and backward outputs to get a single\n",
        "        # context vector per time step. Return (outputs, hidden).\n",
        "        # 1) Embed tokens -> (max_len, batch, emb_dim)\n",
        "        embedded = self.embedding(input_seq)\n",
        "\n",
        "        # 2) Pack (assumes sequences are sorted by length desc; your batch2TrainData does that)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded,\n",
        "            input_lengths.cpu(),   # safer for pack on some setups\n",
        "            enforce_sorted=True\n",
        "        )\n",
        "\n",
        "        # 3) Run through BiGRU\n",
        "        packed_outputs, hidden = self.gru(packed, hidden)\n",
        "\n",
        "        # 4) Unpack -> outputs: (max_len, batch, hidden_size*2)\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "\n",
        "        # 5) Sum bidirectional outputs: (max_len, batch, hidden_size)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "\n",
        "        # 6) Sum bidirectional hidden states too:\n",
        "        # hidden originally: (n_layers*2, batch, hidden_size)\n",
        "        # reshape split directions: forward layers [0:n_layers], backward layers [n_layers:2*n_layers]\n",
        "        hidden = hidden[:self.n_layers] + hidden[self.n_layers:]\n",
        "\n",
        "        return outputs, hidden\n"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9GAbGk0Db-T"
      },
      "source": [
        "#### Attention Layer\n",
        "\n",
        "The [Luong attention mechanism](https://arxiv.org/abs/1508.04025) computes a context vector as a weighted sum of encoder outputs. Three scoring functions are supported:\n",
        "\n",
        "| Method | Formula |\n",
        "|--------|---------|\n",
        "| `dot` | $h_t^\\top \\bar{h}_s$ |\n",
        "| `general` | $h_t^\\top W_a \\bar{h}_s$ |\n",
        "| `concat` | $v_a^\\top \\tanh(W_a [h_t ; \\bar{h}_s])$ |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz6SiP81Db-T"
      },
      "outputs": [],
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        # TODO: For 'general', create a Linear(hidden_size, hidden_size).\n",
        "        # For 'concat', create Linear(hidden_size*2, hidden_size) and a learnable\n",
        "        # parameter vector v of size hidden_size.\n",
        "        # For \"general\": score(h_t, h_s) = h_t^T W_a h_s\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # For \"concat\": score(h_t, h_s) = v_a^T tanh(W_a [h_t ; h_s])\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        # TODO: Compute element-wise product and sum over the last dimension.\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        # TODO: Apply self.attn to encoder_output, then dot with hidden.\n",
        "        energy = self.attn(encoder_output)              # (src_len, batch, hidden)\n",
        "        return torch.sum(hidden * energy, dim=2)        # (src_len, batch)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        # TODO: Expand hidden to match encoder_output shape, concatenate,\n",
        "        # apply self.attn + tanh, then dot with self.v.\n",
        "        src_len = encoder_output.size(0)\n",
        "\n",
        "        # Expand hidden from (1, batch, hidden) -> (src_len, batch, hidden)\n",
        "        hidden_expanded = hidden.expand(src_len, -1, -1)\n",
        "\n",
        "        # Concat on last dim -> (src_len, batch, 2*hidden)\n",
        "        concat_input = torch.cat((hidden_expanded, encoder_output), dim=2)\n",
        "\n",
        "        # Apply linear + tanh -> (src_len, batch, hidden)\n",
        "        energy = torch.tanh(self.attn(concat_input))\n",
        "\n",
        "        # Dot with v: (hidden,) with (src_len, batch, hidden) -> (src_len, batch)\n",
        "        # Use broadcasting: energy * v, then sum over hidden\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # TODO: Dispatch to the correct scoring function based on self.method,\n",
        "        # transpose the energies, and return a softmax probability distribution\n",
        "        # with an added dimension (shape: batch x 1 x src_len).\n",
        "        # 1) Compute attention energies: (src_len, batch)\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        else:  # 'dot'\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # 2) Transpose to (batch, src_len)\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # 3) Softmax over src_len, then unsqueeze -> (batch, 1, src_len)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC4fhJ4KDb-T"
      },
      "source": [
        "#### Decoder\n",
        "\n",
        "The `LuongAttnDecoderRNN` generates one output token per step. It attends to encoder outputs via the `Attn` module, concatenates the context vector with the GRU output, and projects the result to a vocabulary-sized distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmLKIyIpDb-T"
      },
      "outputs": [],
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        # TODO: Define a unidirectional GRU (hidden_size -> hidden_size, n_layers).\n",
        "        emb_dim = embedding.embedding_dim\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            dropout=(0 if n_layers == 1 else dropout),\n",
        "            bidirectional=False\n",
        "        )\n",
        "        # TODO: Define a concat Linear(hidden_size*2, hidden_size) to merge context + GRU output.\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        # TODO: Define an output Linear(hidden_size, output_size) for the vocabulary projection.\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # TODO: Instantiate an Attn(attn_model, hidden_size) attention module.\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # TODO:\n",
        "        # 1. Embed input_step and apply dropout.\n",
        "        # embedded: (1, batch, emb_dim)\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # 2. Run through self.gru to get rnn_output and new hidden state.\n",
        "        # rnn_output: (1, batch, hidden_size)\n",
        "        # hidden:     (n_layers, batch, hidden_size)\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # 3. Compute attention weights over encoder_outputs using self.attn.\n",
        "        # attn_weights: (batch, 1, src_len)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # 4. Compute context vector via batch matrix multiply (bmm).\n",
        "        # encoder_outputs -> (batch, src_len, hidden_size)\n",
        "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
        "        # context: (batch, 1, hidden_size)\n",
        "        context = attn_weights.bmm(encoder_outputs)\n",
        "        # 5. Concatenate rnn_output and context, apply self.concat + tanh.\n",
        "        # rnn_output: (batch, 1, hidden_size)\n",
        "        rnn_output = rnn_output.transpose(0, 1)\n",
        "        # concat_input: (batch, 1, 2*hidden_size)\n",
        "        concat_input = torch.cat((rnn_output, context), dim=2)\n",
        "        # concat_output: (batch, hidden_size)\n",
        "        concat_output = torch.tanh(self.concat(concat_input)).squeeze(1)\n",
        "        # 6. Project to vocabulary size with self.out and apply softmax.\n",
        "        # output: (batch, output_size)\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8rIvbMYDb-T"
      },
      "source": [
        "### Define Training Procedure\n",
        "\n",
        "#### Masked NLL Loss\n",
        "\n",
        "Because sequences are padded to the same length within a batch, we compute loss only over non-padding positions using a binary mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iph2dbvsDb-T"
      },
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    \"\"\"Compute NLL loss over non-padded positions only.\n",
        "\n",
        "    Args:\n",
        "        inp:    (batch, vocab_size) softmax probabilities from the decoder\n",
        "        target: (batch,) ground-truth token indices\n",
        "        mask:   (batch,) boolean mask — True for real tokens, False for PAD\n",
        "    Returns:\n",
        "        loss (scalar tensor), nTotal (int count of real tokens)\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # 1. Count the number of non-padded tokens (mask.sum()).\n",
        "    nTotal = mask.sum()\n",
        "    # 2. Gather the log-probability of the correct token for each item in the batch.\n",
        "    gathered = torch.gather(inp, 1, target.unsqueeze(1)).squeeze(1)\n",
        "    # 3. Select only the masked (real) tokens and take the mean.\n",
        "    loss = -torch.log(gathered + 1e-12)\n",
        "    # 4. Move loss to device and return (loss, nTotal).\n",
        "    loss = loss.masked_select(mask).mean()\n",
        "    # Ensure tensor is on correct device\n",
        "    loss = loss.to(inp.device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8XDDBg0Db-T"
      },
      "source": [
        "#### Single Training Iteration\n",
        "\n",
        "The `train` function performs one forward and backward pass over a single batch. Key techniques:\n",
        "\n",
        "- **Teacher forcing**: with probability `teacher_forcing_ratio`, the ground-truth token is fed as the next decoder input instead of the model's own prediction. Higher values accelerate early convergence but may hurt generalization.\n",
        "- **Gradient clipping**: gradients are clipped to a maximum norm of `clip` to prevent exploding gradients, which are common in RNN training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8zoH2ZfDb-T"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "          encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer,\n",
        "          batch_size, clip, max_length=MAX_LENGTH):\n",
        "    \"\"\"Run one mini-batch forward + backward pass.\n",
        "\n",
        "    Steps:\n",
        "      1. Zero gradients on both optimizers.\n",
        "      2. Move tensors to device (keep lengths on CPU for pack_padded_sequence).\n",
        "      3. Run encoder to get encoder_outputs and encoder_hidden.\n",
        "      4. Initialize decoder_input with SOS tokens (shape: 1 x batch_size).\n",
        "      5. Set decoder_hidden from encoder_hidden[:decoder.n_layers].\n",
        "      6. Decide teacher forcing: if random() < teacher_forcing_ratio use ground truth,\n",
        "         otherwise use the decoder's own top-1 prediction as the next input.\n",
        "      7. Loop over max_target_len steps, accumulate maskNLLLoss.\n",
        "      8. loss.backward(), clip gradients for both encoder and decoder, step optimizers.\n",
        "    Returns:\n",
        "      Average loss per real token (float).\n",
        "    \"\"\"\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Move tensors to device (lengths stay on CPU)\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    # Encoder forward pass\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Initialize decoder input with SOS tokens\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Initialize decoder hidden state from encoder\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Decide teacher forcing\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Decode one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # Teacher forcing: next input is ground truth\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "\n",
        "            mask_loss, nTotal = maskNLLLoss(\n",
        "                decoder_output,\n",
        "                target_variable[t],\n",
        "                mask[t]\n",
        "            )\n",
        "\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # No teacher forcing: use model prediction\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.transpose(0, 1).detach()\n",
        "\n",
        "            mask_loss, nTotal = maskNLLLoss(\n",
        "                decoder_output,\n",
        "                target_variable[t],\n",
        "                mask[t]\n",
        "            )\n",
        "\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Optimizer step\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Return average loss per real token\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd399FoCDb-T"
      },
      "source": [
        "#### Training Loop\n",
        "\n",
        "`trainIters` manages the full training loop: printing average loss every `print_every` iterations and saving checkpoints every `save_every` iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKPtWxwcDb-T"
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder,\n",
        "               encoder_optimizer, decoder_optimizer, embedding,\n",
        "               encoder_n_layers, decoder_n_layers, save_dir,\n",
        "               n_iteration, batch_size, print_every, save_every,\n",
        "               clip, corpus_name, loadFilename):\n",
        "\n",
        "    training_batches = [\n",
        "        batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "        for _ in range(n_iteration)\n",
        "    ]\n",
        "\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "                     encoder, decoder, embedding,\n",
        "                     encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
        "                iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        if iteration % save_every == 0:\n",
        "            directory = os.path.join(\n",
        "                save_dir, model_name, corpus_name,\n",
        "                '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size)\n",
        "            )\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXCiT1hODb-T"
      },
      "source": [
        "### Define Evaluation\n",
        "\n",
        "#### Greedy Search Decoder\n",
        "\n",
        "At inference time we use greedy decoding: at each step, select the token with the highest probability and feed it as input to the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFSTsXMDb-T"
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        \"\"\"Greedily decode up to max_length tokens.\n",
        "\n",
        "        Steps:\n",
        "          1. Run self.encoder to get encoder_outputs and encoder_hidden.\n",
        "          2. Initialize decoder_hidden and decoder_input (SOS token).\n",
        "          3. At each step: run self.decoder, take the argmax token,\n",
        "             append it to all_tokens and its score to all_scores.\n",
        "          4. Feed the chosen token back as the next decoder input.\n",
        "        Returns:\n",
        "          (all_tokens, all_scores) — both 1-D tensors of length max_length.\n",
        "        \"\"\"\n",
        "        # 1) Encoder forward\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "\n",
        "        # 2) Prepare decoder initial hidden state (match decoder layers)\n",
        "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
        "\n",
        "        # 3) Initialize decoder input with SOS\n",
        "        decoder_input = torch.LongTensor([[SOS_token]]).to(input_seq.device)\n",
        "\n",
        "        all_tokens = torch.zeros([0], dtype=torch.long, device=input_seq.device)\n",
        "        all_scores = torch.zeros([0], dtype=torch.float, device=input_seq.device)\n",
        "\n",
        "        # 4) Greedy decode\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "\n",
        "            # decoder_output: (batch=1, vocab)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # decoder_input: (1,) -> shape to (1,1) for next step\n",
        "            decoder_input = decoder_input.view(1, 1)\n",
        "\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input.squeeze(0)), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "\n",
        "        return all_tokens, all_scores\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    \"\"\"Convert a normalized sentence string to a list of decoded word strings.\"\"\"\n",
        "    # TODO:\n",
        "    # 1. Convert sentence to index tensor (indexesFromSentence), get lengths.\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    lengths = torch.tensor([len(indexes_batch[0])])\n",
        "    # 2. Transpose to (seq_len, 1) for the encoder.\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # 3. Run searcher to get token indices.\n",
        "    input_batch = input_batch.to(device)\n",
        "    # 4. Map indices back to words via voc.index2word.\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    \"\"\"Interactive loop: read input from stdin, print bot response. Type 'q' to quit.\"\"\"\n",
        "    input_sentence = ''\n",
        "    while True:\n",
        "        try:\n",
        "            input_sentence = input('> ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit':\n",
        "                break\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")\n"
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNiYNigzDb-T"
      },
      "source": [
        "### Model Initialization & Run Training\n",
        "\n",
        "Configure the model hyperparameters, build the encoder and decoder, initialize optimizers, and start training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l34sPj5RDb-T",
        "outputId": "6c91af7c-3238-4d63-806d-09411897e1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models built and ready to go!\n",
            "Initializing ...\n",
            "Training...\n",
            "Iteration: 100; Percent complete: 2.5%; Average loss: 5.0112\n",
            "Iteration: 200; Percent complete: 5.0%; Average loss: 4.2540\n",
            "Iteration: 300; Percent complete: 7.5%; Average loss: 3.9635\n",
            "Iteration: 400; Percent complete: 10.0%; Average loss: 3.8652\n",
            "Iteration: 500; Percent complete: 12.5%; Average loss: 3.7658\n",
            "Iteration: 600; Percent complete: 15.0%; Average loss: 3.6954\n",
            "Iteration: 700; Percent complete: 17.5%; Average loss: 3.6515\n",
            "Iteration: 800; Percent complete: 20.0%; Average loss: 3.5729\n",
            "Iteration: 900; Percent complete: 22.5%; Average loss: 3.5492\n",
            "Iteration: 1000; Percent complete: 25.0%; Average loss: 3.5084\n",
            "Iteration: 1100; Percent complete: 27.5%; Average loss: 3.4693\n",
            "Iteration: 1200; Percent complete: 30.0%; Average loss: 3.4304\n",
            "Iteration: 1300; Percent complete: 32.5%; Average loss: 3.3885\n",
            "Iteration: 1400; Percent complete: 35.0%; Average loss: 3.3457\n",
            "Iteration: 1500; Percent complete: 37.5%; Average loss: 3.3157\n",
            "Iteration: 1600; Percent complete: 40.0%; Average loss: 3.2477\n",
            "Iteration: 1700; Percent complete: 42.5%; Average loss: 3.2795\n",
            "Iteration: 1800; Percent complete: 45.0%; Average loss: 3.2048\n",
            "Iteration: 1900; Percent complete: 47.5%; Average loss: 3.1665\n",
            "Iteration: 2000; Percent complete: 50.0%; Average loss: 3.1703\n",
            "Iteration: 2100; Percent complete: 52.5%; Average loss: 3.1515\n",
            "Iteration: 2200; Percent complete: 55.0%; Average loss: 3.0927\n",
            "Iteration: 2300; Percent complete: 57.5%; Average loss: 3.0805\n",
            "Iteration: 2400; Percent complete: 60.0%; Average loss: 3.0469\n",
            "Iteration: 2500; Percent complete: 62.5%; Average loss: 3.0259\n",
            "Iteration: 2600; Percent complete: 65.0%; Average loss: 3.0019\n",
            "Iteration: 2700; Percent complete: 67.5%; Average loss: 2.9534\n",
            "Iteration: 2800; Percent complete: 70.0%; Average loss: 2.9370\n",
            "Iteration: 2900; Percent complete: 72.5%; Average loss: 2.8943\n",
            "Iteration: 3000; Percent complete: 75.0%; Average loss: 2.8922\n",
            "Iteration: 3100; Percent complete: 77.5%; Average loss: 2.8669\n",
            "Iteration: 3200; Percent complete: 80.0%; Average loss: 2.8209\n",
            "Iteration: 3300; Percent complete: 82.5%; Average loss: 2.8083\n",
            "Iteration: 3400; Percent complete: 85.0%; Average loss: 2.7947\n",
            "Iteration: 3500; Percent complete: 87.5%; Average loss: 2.7451\n",
            "Iteration: 3600; Percent complete: 90.0%; Average loss: 2.7274\n",
            "Iteration: 3700; Percent complete: 92.5%; Average loss: 2.7222\n",
            "Iteration: 3800; Percent complete: 95.0%; Average loss: 2.6823\n",
            "Iteration: 3900; Percent complete: 97.5%; Average loss: 2.6272\n",
            "Iteration: 4000; Percent complete: 100.0%; Average loss: 2.6027\n"
          ]
        }
      ],
      "source": [
        "# ---- Model hyperparameters ----\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'        # attention scoring: 'dot', 'general', or 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# ---- Training hyperparameters ----\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 100\n",
        "save_every = 500\n",
        "\n",
        "loadFilename = None  # set to a .tar checkpoint path to resume training\n",
        "\n",
        "# ---- Build models ----\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size,\n",
        "                               voc.num_words, decoder_n_layers, dropout)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')\n",
        "\n",
        "# ---- Optimizers ----\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(),\n",
        "                               lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "# ---- Train ----\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "trainIters(model_name, voc, pairs, encoder, decoder,\n",
        "           encoder_optimizer, decoder_optimizer, embedding,\n",
        "           encoder_n_layers, decoder_n_layers, save_dir,\n",
        "           n_iteration, batch_size, print_every, save_every,\n",
        "           clip, corpus_name, loadFilename)"
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc3PkqshDb-T"
      },
      "source": [
        "### Interact with the Chatbot\n",
        "\n",
        "Switch models to evaluation mode and start a conversation. Type `q` to quit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQoDAZPjDb-T",
        "outputId": "4fb058f5-c896-449c-d30d-5a249af0d149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> hello\n",
            "Bot: hello . . . . .\n",
            "> what are you\n",
            "Bot: i m sorry . . . .\n",
            "> why sorry\n",
            "Bot: why ? my heart is mine .\n",
            "> that is deep\n",
            "Bot: yes . . . . .\n",
            "> why so deep?\n",
            "Bot: i don t know . . .\n",
            "> are you confused?\n",
            "Bot: yes . . . . .\n",
            "> why are you confused\n",
            "Bot: i m not a good idea . .\n",
            "> no you are a good idea\n",
            "Bot: no i m not\n",
            "> why are you not\n",
            "Bot: why not ? you re dead . .\n",
            "> what the fuck?\n",
            "Bot: what ? the bay . . .\n",
            "> I am not dead\n",
            "Bot: you re not ? you re dead .\n",
            "> I AM NOT DEAD\n",
            "Bot: you re not ? you re dead .\n",
            "> FUCK OFF\n",
            "Bot: you re a girl . . .\n",
            "> q\n"
          ]
        }
      ],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6Jqg8wZDb-T"
      },
      "source": [
        "---\n",
        "## Task 2: Learn Weights & Biases (W&B) — No Points, Required for Tasks 3–5\n",
        "\n",
        "Before proceeding, watch the [Hyperparameter Sweeps with W&B video tutorial](https://www.youtube.com/watch?v=9zrmUIlScdY) and review the [accompanying Colab notebook](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb).\n",
        "\n",
        "Then install and authenticate the W&B library below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EBfRs2RDb-T",
        "outputId": "bf769db6-5865-4216-8518-59a77a166bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create a new API key at: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Store your API key securely and do not share it.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste your API key and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanishpatel01\u001b[0m (\u001b[33mtanishpatel01-columbia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# wandb was already installed with a version pin in the setup cell above\n",
        "import wandb\n",
        "wandb.login()  # Enter your API key when prompted — sign up free at https://wandb.ai/site"
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua6jV4UnDb-U"
      },
      "source": [
        "---\n",
        "## Task 3 [5 points]: Create a W&B Sweep Configuration\n",
        "\n",
        "Define a sweep configuration using the **W&B Random Search** strategy over the following hyperparameters:\n",
        "\n",
        "| Hyperparameter | Search values |\n",
        "|---|---|\n",
        "| `learning_rate` | 0.0001, 0.00025, 0.0005, 0.001 |\n",
        "| `optimizer` | adam, sgd |\n",
        "| `clip` | 0, 25, 50, 100 |\n",
        "| `teacher_forcing_ratio` | 0, 0.5, 1.0 |\n",
        "| `decoder_learning_ratio` | 1.0, 3.0, 5.0, 10.0 |\n",
        "\n",
        "The sweep should **minimize** the metric `train_loss`.\n",
        "\n",
        "**Hints:**\n",
        "- Use `method: \"random\"` and specify each hyperparameter under `parameters` with a `values` list.\n",
        "- Instrument your training loop with `wandb.init(config=...)` and `wandb.log({\"train_loss\": ...})`.\n",
        "- Register your sweep with `sweep_id = wandb.sweep(sweep_config, project=\"chatbot-sweep\")`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTGPG0GiDb-U",
        "outputId": "7a89909d-8287-4fd5-ac69-f0036508ec68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: fny2xfoy\n",
            "Sweep URL: https://wandb.ai/tanishpatel01-columbia/chatbot-sweep/sweeps/fny2xfoy\n",
            "Sweep ID: fny2xfoy\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define sweep_config using W&B random search over the hyperparameters listed above.\n",
        "# Then register the sweep:\n",
        "#   sweep_id = wandb.sweep(sweep_config, project='chatbot-sweep')\n",
        "sweep_config = {\n",
        "    \"method\": \"random\",\n",
        "    \"metric\": {\"name\": \"train_loss\", \"goal\": \"minimize\"},\n",
        "    \"parameters\": {\n",
        "        \"learning_rate\": {\"values\": [1e-4, 2.5e-4, 5e-4, 1e-3]},\n",
        "        \"optimizer\": {\"values\": [\"adam\", \"sgd\"]},\n",
        "        \"clip\": {\"values\": [0, 25, 50, 100]},\n",
        "        \"teacher_forcing_ratio\": {\"values\": [0.0, 0.5, 1.0]},\n",
        "        \"decoder_learning_ratio\": {\"values\": [1.0, 3.0, 5.0, 10.0]},\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"chatbot-sweep\")\n",
        "print(\"Sweep ID:\", sweep_id)\n"
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtP27ie-Db-U"
      },
      "source": [
        "### Instrument the Training Loop for W&B\n",
        "\n",
        "Write a `train_sweep()` function that:\n",
        "1. Calls `wandb.init()` to start a new run\n",
        "2. Reads hyperparameter values from `wandb.config` (e.g. `wandb.config.learning_rate`)\n",
        "3. Builds the models and optimizers using those values\n",
        "4. Calls your training loop and logs `train_loss` at each `print_every` step with `wandb.log()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsKv67wADb-U"
      },
      "outputs": [],
      "source": [
        "def train_sweep():\n",
        "    \"\"\"Single sweep run invoked by the W&B agent.\n",
        "\n",
        "    Steps:\n",
        "      1. Call wandb.init() (use as a context manager).\n",
        "      2. Read hyperparameters from wandb.config\n",
        "         (learning_rate, optimizer, clip, teacher_forcing_ratio, decoder_learning_ratio).\n",
        "      3. Build fresh encoder/decoder and optimizers using those values.\n",
        "      4. Run the training loop; after every print_every iterations call\n",
        "         wandb.log({'train_loss': avg_loss, 'iteration': iteration}).\n",
        "    \"\"\"\n",
        "    with wandb.init() as run:\n",
        "\n",
        "        config = wandb.config\n",
        "\n",
        "        # 1️⃣ Read hyperparameters\n",
        "        learning_rate = config.learning_rate\n",
        "        optimizer_name = config.optimizer\n",
        "        clip = config.clip\n",
        "        teacher_forcing_ratio = config.teacher_forcing_ratio\n",
        "        decoder_learning_ratio = config.decoder_learning_ratio\n",
        "\n",
        "        # Make teacher_forcing_ratio global (used inside train())\n",
        "        teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "        # 2️⃣ Build fresh models\n",
        "        hidden_size = 500\n",
        "        encoder_n_layers = 2\n",
        "        decoder_n_layers = 2\n",
        "        dropout = 0.1\n",
        "\n",
        "        embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "\n",
        "        encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout).to(device)\n",
        "        decoder = LuongAttnDecoderRNN(\n",
        "            attn_model=\"dot\",\n",
        "            embedding=embedding,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=voc.num_words,\n",
        "            n_layers=decoder_n_layers,\n",
        "            dropout=dropout\n",
        "        ).to(device)\n",
        "\n",
        "        # 3️⃣ Optimizers\n",
        "        if optimizer_name.lower() == \"adam\":\n",
        "            encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "            decoder_optimizer = optim.Adam(\n",
        "                decoder.parameters(),\n",
        "                lr=learning_rate * decoder_learning_ratio\n",
        "            )\n",
        "        else:  # SGD\n",
        "            encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "            decoder_optimizer = optim.SGD(\n",
        "                decoder.parameters(),\n",
        "                lr=learning_rate * decoder_learning_ratio\n",
        "            )\n",
        "\n",
        "        # 4️⃣ Training setup\n",
        "        n_iteration = 4000\n",
        "        batch_size = 64\n",
        "        print_every = 100\n",
        "\n",
        "        print_loss = 0\n",
        "\n",
        "        training_batches = [\n",
        "            batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "            for _ in range(n_iteration)\n",
        "        ]\n",
        "\n",
        "        # 5️⃣ Training loop\n",
        "        for iteration in range(1, n_iteration + 1):\n",
        "\n",
        "            training_batch = training_batches[iteration - 1]\n",
        "            input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "            loss = train(\n",
        "                input_variable, lengths,\n",
        "                target_variable, mask, max_target_len,\n",
        "                encoder, decoder, embedding,\n",
        "                encoder_optimizer, decoder_optimizer,\n",
        "                batch_size, clip\n",
        "            )\n",
        "\n",
        "            print_loss += loss\n",
        "\n",
        "            if iteration % print_every == 0:\n",
        "                avg_loss = print_loss / print_every\n",
        "                print_loss = 0\n",
        "\n",
        "                wandb.log({\n",
        "                    \"train_loss\": avg_loss,\n",
        "                    \"iteration\": iteration\n",
        "                })"
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td2-IDY7Db-U"
      },
      "source": [
        "---\n",
        "## Task 4 [5 points]: Run Hyperparameter Sweeps on GPU Colab\n",
        "\n",
        "Launch a W&B sweep agent on a **GPU-enabled** Colab runtime (Runtime > Change runtime type > T4 GPU). The agent will automatically sample configurations and execute `train_sweep()` for each one.\n",
        "\n",
        "- Run at least **10 sweep trials** to cover a meaningful portion of the search space.\n",
        "- Monitor results live in the [W&B console](https://wandb.ai).\n",
        "- Paste your W&B project URL or a screenshot of the sweep results dashboard below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifVXqJEvDb-U"
      },
      "outputs": [],
      "source": [
        "# TODO: Launch the W&B sweep agent.\n",
        "# Run at least 10 trials on a GPU-enabled Colab runtime.\n",
        "#   wandb.agent(sweep_id, function=train_sweep, count=10)\n",
        "# YOUR CODE HERE\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkAZtN5rDb-U"
      },
      "source": [
        "**W&B Project Link / Screenshot:**\n",
        "\n",
        "*Paste your W&B sweep URL here (e.g. https://wandb.ai/\\<username\\>/chatbot-sweep/sweeps/\\<sweep-id\\>) or embed a screenshot of the parallel coordinates / loss curves.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Rms2rfDb-U"
      },
      "source": [
        "---\n",
        "## Task 5 [10 points]: Analysis of Best Hyperparameters & Feature Importance\n",
        "\n",
        "After completing your sweeps, answer all four questions below.\n",
        "\n",
        "### 5a. Best Configuration\n",
        "Report the hyperparameter values that achieved the **lowest `train_loss`** across all sweep runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAmauhvTDb-U"
      },
      "outputs": [],
      "source": [
        "# (Optional) Retrieve the best run programmatically via the W&B API:\n",
        "#\n",
        "# api = wandb.Api()\n",
        "# runs = api.runs(\"<your-entity>/chatbot-sweep\")\n",
        "# best_run = min(runs, key=lambda r: r.summary.get(\"train_loss\", float(\"inf\")))\n",
        "# print(\"Best config:\", best_run.config)\n",
        "# print(\"Best loss: \", best_run.summary[\"train_loss\"])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRyrjuCJDb-U"
      },
      "source": [
        "**Best hyperparameter configuration:**\n",
        "\n",
        "| Hyperparameter | Best Value |\n",
        "|---|---|\n",
        "| `learning_rate` | *(your answer)* |\n",
        "| `optimizer` | *(your answer)* |\n",
        "| `clip` | *(your answer)* |\n",
        "| `teacher_forcing_ratio` | *(your answer)* |\n",
        "| `decoder_learning_ratio` | *(your answer)* |\n",
        "| **Best `train_loss`** | *(your answer)* |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byRd0tXkDb-U"
      },
      "source": [
        "### 5b. Feature Importance\n",
        "\n",
        "Use the **W&B feature importance panel** (available in the sweep UI under \"Parameter Importance\") to identify which hyperparameters had the greatest and least impact on `train_loss`.\n",
        "\n",
        "*Paste a screenshot of the feature importance chart here and list the top-3 most important hyperparameters.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ5UYXxoDb-U"
      },
      "source": [
        "### 5c. Convergence Analysis\n",
        "\n",
        "Explain, in your own words, **why** the top hyperparameters from 5b affect model convergence. Address each of the following:\n",
        "\n",
        "- **Learning rate** — how does its magnitude affect gradient update steps and the risk of overshooting minima?\n",
        "- **Optimizer choice (Adam vs SGD)** — how do adaptive vs. fixed learning rates influence training on sparse/noisy sequence data?\n",
        "- **Gradient clipping (`clip`)** — why does clipping stabilize RNN training, and what happens when `clip=0` (no clipping) or `clip=100` (very loose)?\n",
        "- **Teacher forcing ratio** — how does the tradeoff between training with ground-truth vs. predicted tokens affect convergence speed and exposure bias?\n",
        "- **Decoder learning ratio** — why might the decoder benefit from a different learning rate than the encoder?\n",
        "\n",
        "*Write your analysis here (aim for 200–400 words).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "verHT286Db-U"
      },
      "source": [
        "### 5d. Chatbot Quality\n",
        "\n",
        "Load the best checkpoint and interact with the chatbot. Report **at least 5 example exchanges** and briefly comment on the quality of the responses.\n",
        "\n",
        "```\n",
        "> <your input>\n",
        "Bot: <model output>\n",
        "\n",
        "> <your input>\n",
        "Bot: <model output>\n",
        "```\n",
        "\n",
        "*Replace the template above with your actual exchanges.*"
      ]
    }
  ]
}