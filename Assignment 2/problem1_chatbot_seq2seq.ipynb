{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Training a Simple Chatbot using a Seq-to-Seq Model (25 points)\n",
    "\n",
    "You will train a simple chatbot using movie scripts from the Cornell Movie Dialogs Corpus based on the [PyTorch Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html).\n",
    "\n",
    "This tutorial allows you to train a recurrent sequence-to-sequence model. You will learn the following concepts:\n",
    "\n",
    "- Handle loading and pre-processing of [the Cornell Movie-Dialogs Corpus dataset](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "- Implement a sequence-to-sequence model with [Luong attention mechanism(s)](https://arxiv.org/abs/1508.04025)\n",
    "- Jointly train encoder and decoder models using mini-batches\n",
    "- Implement greedy-search decoding module\n",
    "- Interact with the trained chatbot\n",
    "\n",
    "---\n",
    "\n",
    "## Scoring Breakdown\n",
    "\n",
    "| Task | Points |\n",
    "|------|--------|\n",
    "| Task 1: Run the tutorial end-to-end in Colab | 5 |\n",
    "| Task 3: Create W&B sweep configuration | 5 |\n",
    "| Task 4: Run hyperparameter sweeps on GPU Colab | 5 |\n",
    "| Task 5: Analysis of best hyperparameters & feature importance | 10 |\n",
    "| **Total** | **25** |\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "- [The Cornell Movie Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "- [Hyperparameter sweeps with Weights and Biases (video tutorial)](https://www.youtube.com/watch?v=9zrmUIlScdY)\n",
    "- [Sample Google Colab project for W&B sweeps](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb)\n",
    "- [Weights and Biases Website](https://wandb.ai/site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 [5 points]: Run the Tutorial End-to-End\n",
    "\n",
    "Make a copy of the [PyTorch Chatbot Tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html) notebook, follow the instructions to train and evaluate the chatbot model in your **Google Colab** environment (GPU recommended).\n",
    "\n",
    "The tutorial code is provided below as your starting point. Run each cell in order and verify that the model trains successfully and you can interact with the chatbot at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Install Dependencies and Download Data\n",
    "\n",
    "The Cornell Movie-Dialogs Corpus must be downloaded before running the tutorial. The dataset is available via [ConvoKit](https://convokit.cornell.edu/documentation/movie.html)."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies (torch is pre-installed in Colab; run the %%writefile cell above first)\n!pip install -r requirements.txt -q\n\n# Download the Cornell Movie-Dialogs Corpus via ConvoKit\nimport convokit\ncorpus = convokit.Corpus(filename=convokit.download(\"movie-corpus\"))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages (run once in Colab)\n# Requires PyTorch >= 2.4.0 (pre-installed in Colab; verify with: import torch; print(torch.__version__))\n!pip install \"convokit>=3.0,<4.0\" -q\n!pip install \"wandb>=0.18\" -q\n\n# Download the Cornell Movie-Dialogs Corpus via ConvoKit\nimport convokit\ncorpus = convokit.Corpus(filename=convokit.download(\"movie-corpus\"))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\nimport json\n\nassert torch.__version__ >= \"2.4\", f\"PyTorch >= 2.4 required, got {torch.__version__}\"\n\nif torch.accelerator.is_available():\n    device = torch.accelerator.current_accelerator().type\nelif torch.backends.mps.is_available():\n    device = \"mps\"\nelse:\n    device = \"cpu\"\nprint(f\"Using {device} device\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Preprocess Data\n",
    "\n",
    "The Cornell Movie-Dialogs Corpus is stored in `utterances.jsonl` format. We parse the raw file to extract consecutive question-answer sentence pairs from each conversation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLinesAndConversations(fileName):\n",
    "    lines = {}\n",
    "    conversations = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            lineJson = json.loads(line)\n",
    "            lineObj = {}\n",
    "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
    "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
    "            lineObj[\"text\"] = lineJson[\"text\"]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "\n",
    "            if lineJson[\"conversation_id\"] not in conversations:\n",
    "                convObj = {}\n",
    "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
    "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
    "                convObj[\"lines\"] = [lineObj]\n",
    "            else:\n",
    "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
    "                convObj[\"lines\"].insert(0, lineObj)\n",
    "            conversations[convObj[\"conversationID\"]] = convObj\n",
    "\n",
    "    return lines, conversations\n",
    "\n",
    "\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations.values():\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Formatted Data File\n",
    "\n",
    "Parse the corpus and write tab-separated input/output pairs to a text file for training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths — update corpus_path to where ConvoKit downloaded the dataset\n",
    "corpus_name = \"movie-corpus\"\n",
    "corpus_path = os.path.join(\"/root/.convokit/saved-corpora\", corpus_name)\n",
    "datafile = os.path.join(corpus_path, \"formatted_movie_lines.txt\")\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines, conversations = loadLinesAndConversations(\n",
    "    os.path.join(corpus_path, \"utterances.jsonl\")\n",
    ")\n",
    "\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in extractSentencePairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "print(\"\\nSample lines from file:\")\n",
    "with open(datafile, 'rb') as f:\n",
    "    lines_sample = f.readlines()\n",
    "for line in lines_sample[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary Class\n",
    "\n",
    "The `Voc` class maintains word-to-index and index-to-word mappings. Three special tokens are reserved:\n",
    "- `PAD` (0): padding token used to equalize batch sequence lengths\n",
    "- `SOS` (1): start-of-sequence token fed as the first decoder input\n",
    "- `EOS` (2): end-of-sequence token appended to every target sequence"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Normalization & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length (in words) to consider\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([\\[.!?\\]])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "voc, pairs = loadPrepareData(corpus_name, corpus_name, datafile, save_dir)\n",
    "print(\"\\nSample pairs:\")\n",
    "for pair in pairs[:5]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim Rare Words\n",
    "\n",
    "Remove words appearing fewer than `MIN_COUNT` times to reduce vocabulary size and improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 3\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    voc.trim(MIN_COUNT)\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
    "        len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "    ))\n",
    "    return keep_pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Models\n",
    "\n",
    "Convert sentence pairs into padded tensors suitable for batch training. Sequences in a batch are padded to the same length, and a binary mask is created so that the loss function ignores padding positions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    \"\"\"Returns padded input sequence tensor and lengths.\"\"\"\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    \"\"\"Returns padded target sequence tensor, mask, and max target length.\"\"\"\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    \"\"\"Returns all items for a given batch of pairs.\"\"\"\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Sanity check\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "The encoder is a bidirectional GRU. For each input token it produces a hidden state; the forward and backward outputs are **summed** to form a single context vector per time step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # TODO: Define a bidirectional GRU with the given hidden_size, n_layers, and dropout.\n",
    "        # Use nn.GRU — remember to set bidirectional=True.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # TODO: Embed input_seq, pack the padded sequence, run through the GRU,\n",
    "        # unpack, then SUM the forward and backward outputs to get a single\n",
    "        # context vector per time step. Return (outputs, hidden).\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Layer\n",
    "\n",
    "The [Luong attention mechanism](https://arxiv.org/abs/1508.04025) computes a context vector as a weighted sum of encoder outputs. Three scoring functions are supported:\n",
    "\n",
    "| Method | Formula |\n",
    "|--------|---------|\n",
    "| `dot` | $h_t^\\top \\bar{h}_s$ |\n",
    "| `general` | $h_t^\\top W_a \\bar{h}_s$ |\n",
    "| `concat` | $v_a^\\top \\tanh(W_a [h_t ; \\bar{h}_s])$ |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        # TODO: For 'general', create a Linear(hidden_size, hidden_size).\n",
    "        # For 'concat', create Linear(hidden_size*2, hidden_size) and a learnable\n",
    "        # parameter vector v of size hidden_size.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        # TODO: Compute element-wise product and sum over the last dimension.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        # TODO: Apply self.attn to encoder_output, then dot with hidden.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        # TODO: Expand hidden to match encoder_output shape, concatenate,\n",
    "        # apply self.attn + tanh, then dot with self.v.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # TODO: Dispatch to the correct scoring function based on self.method,\n",
    "        # transpose the energies, and return a softmax probability distribution\n",
    "        # with an added dimension (shape: batch x 1 x src_len).\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The `LuongAttnDecoderRNN` generates one output token per step. It attends to encoder outputs via the `Attn` module, concatenates the context vector with the GRU output, and projects the result to a vocabulary-sized distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        # TODO: Define a unidirectional GRU (hidden_size -> hidden_size, n_layers).\n",
    "        # TODO: Define a concat Linear(hidden_size*2, hidden_size) to merge context + GRU output.\n",
    "        # TODO: Define an output Linear(hidden_size, output_size) for the vocabulary projection.\n",
    "        # TODO: Instantiate an Attn(attn_model, hidden_size) attention module.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # TODO:\n",
    "        # 1. Embed input_step and apply dropout.\n",
    "        # 2. Run through self.gru to get rnn_output and new hidden state.\n",
    "        # 3. Compute attention weights over encoder_outputs using self.attn.\n",
    "        # 4. Compute context vector via batch matrix multiply (bmm).\n",
    "        # 5. Concatenate rnn_output and context, apply self.concat + tanh.\n",
    "        # 6. Project to vocabulary size with self.out and apply softmax.\n",
    "        # Return (output, hidden).\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Training Procedure\n",
    "\n",
    "#### Masked NLL Loss\n",
    "\n",
    "Because sequences are padded to the same length within a batch, we compute loss only over non-padding positions using a binary mask."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    \"\"\"Compute NLL loss over non-padded positions only.\n",
    "\n",
    "    Args:\n",
    "        inp:    (batch, vocab_size) softmax probabilities from the decoder\n",
    "        target: (batch,) ground-truth token indices\n",
    "        mask:   (batch,) boolean mask — True for real tokens, False for PAD\n",
    "    Returns:\n",
    "        loss (scalar tensor), nTotal (int count of real tokens)\n",
    "    \"\"\"\n",
    "    # TODO:\n",
    "    # 1. Count the number of non-padded tokens (mask.sum()).\n",
    "    # 2. Gather the log-probability of the correct token for each item in the batch.\n",
    "    # 3. Select only the masked (real) tokens and take the mean.\n",
    "    # 4. Move loss to device and return (loss, nTotal).\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Training Iteration\n",
    "\n",
    "The `train` function performs one forward and backward pass over a single batch. Key techniques:\n",
    "\n",
    "- **Teacher forcing**: with probability `teacher_forcing_ratio`, the ground-truth token is fed as the next decoder input instead of the model's own prediction. Higher values accelerate early convergence but may hurt generalization.\n",
    "- **Gradient clipping**: gradients are clipped to a maximum norm of `clip` to prevent exploding gradients, which are common in RNN training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
    "          encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer,\n",
    "          batch_size, clip, max_length=MAX_LENGTH):\n",
    "    \"\"\"Run one mini-batch forward + backward pass.\n",
    "\n",
    "    Steps:\n",
    "      1. Zero gradients on both optimizers.\n",
    "      2. Move tensors to device (keep lengths on CPU for pack_padded_sequence).\n",
    "      3. Run encoder to get encoder_outputs and encoder_hidden.\n",
    "      4. Initialize decoder_input with SOS tokens (shape: 1 x batch_size).\n",
    "      5. Set decoder_hidden from encoder_hidden[:decoder.n_layers].\n",
    "      6. Decide teacher forcing: if random() < teacher_forcing_ratio use ground truth,\n",
    "         otherwise use the decoder's own top-1 prediction as the next input.\n",
    "      7. Loop over max_target_len steps, accumulate maskNLLLoss.\n",
    "      8. loss.backward(), clip gradients for both encoder and decoder, step optimizers.\n",
    "    Returns:\n",
    "      Average loss per real token (float).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "`trainIters` manages the full training loop: printing average loss every `print_every` iterations and saving checkpoints every `save_every` iterations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder,\n",
    "               encoder_optimizer, decoder_optimizer, embedding,\n",
    "               encoder_n_layers, decoder_n_layers, save_dir,\n",
    "               n_iteration, batch_size, print_every, save_every,\n",
    "               clip, corpus_name, loadFilename):\n",
    "\n",
    "    training_batches = [\n",
    "        batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "        for _ in range(n_iteration)\n",
    "    ]\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
    "                     encoder, decoder, embedding,\n",
    "                     encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
    "                iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir, model_name, corpus_name,\n",
    "                '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size)\n",
    "            )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation\n",
    "\n",
    "#### Greedy Search Decoder\n",
    "\n",
    "At inference time we use greedy decoding: at each step, select the token with the highest probability and feed it as input to the next step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \"\"\"Greedily decode up to max_length tokens.\n",
    "\n",
    "        Steps:\n",
    "          1. Run self.encoder to get encoder_outputs and encoder_hidden.\n",
    "          2. Initialize decoder_hidden and decoder_input (SOS token).\n",
    "          3. At each step: run self.decoder, take the argmax token,\n",
    "             append it to all_tokens and its score to all_scores.\n",
    "          4. Feed the chosen token back as the next decoder input.\n",
    "        Returns:\n",
    "          (all_tokens, all_scores) — both 1-D tensors of length max_length.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    \"\"\"Convert a normalized sentence string to a list of decoded word strings.\"\"\"\n",
    "    # TODO:\n",
    "    # 1. Convert sentence to index tensor (indexesFromSentence), get lengths.\n",
    "    # 2. Transpose to (seq_len, 1) for the encoder.\n",
    "    # 3. Run searcher to get token indices.\n",
    "    # 4. Map indices back to words via voc.index2word.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    \"\"\"Interactive loop: read input from stdin, print bot response. Type 'q' to quit.\"\"\"\n",
    "    input_sentence = ''\n",
    "    while True:\n",
    "        try:\n",
    "            input_sentence = input('> ')\n",
    "            if input_sentence == 'q' or input_sentence == 'quit':\n",
    "                break\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization & Run Training\n",
    "\n",
    "Configure the model hyperparameters, build the encoder and decoder, initialize optimizers, and start training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model hyperparameters ----\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'        # attention scoring: 'dot', 'general', or 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# ---- Training hyperparameters ----\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 100\n",
    "save_every = 500\n",
    "\n",
    "loadFilename = None  # set to a .tar checkpoint path to resume training\n",
    "\n",
    "# ---- Build models ----\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size,\n",
    "                               voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')\n",
    "\n",
    "# ---- Optimizers ----\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(),\n",
    "                               lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "# ---- Train ----\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "trainIters(model_name, voc, pairs, encoder, decoder,\n",
    "           encoder_optimizer, decoder_optimizer, embedding,\n",
    "           encoder_n_layers, decoder_n_layers, save_dir,\n",
    "           n_iteration, batch_size, print_every, save_every,\n",
    "           clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the Chatbot\n",
    "\n",
    "Switch models to evaluation mode and start a conversation. Type `q` to quit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Learn Weights & Biases (W&B) — No Points, Required for Tasks 3–5\n",
    "\n",
    "Before proceeding, watch the [Hyperparameter Sweeps with W&B video tutorial](https://www.youtube.com/watch?v=9zrmUIlScdY) and review the [accompanying Colab notebook](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb).\n",
    "\n",
    "Then install and authenticate the W&B library below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# wandb was already installed with a version pin in the setup cell above\nimport wandb\nwandb.login()  # Enter your API key when prompted — sign up free at https://wandb.ai/site",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 [5 points]: Create a W&B Sweep Configuration\n",
    "\n",
    "Define a sweep configuration using the **W&B Random Search** strategy over the following hyperparameters:\n",
    "\n",
    "| Hyperparameter | Search values |\n",
    "|---|---|\n",
    "| `learning_rate` | 0.0001, 0.00025, 0.0005, 0.001 |\n",
    "| `optimizer` | adam, sgd |\n",
    "| `clip` | 0, 25, 50, 100 |\n",
    "| `teacher_forcing_ratio` | 0, 0.5, 1.0 |\n",
    "| `decoder_learning_ratio` | 1.0, 3.0, 5.0, 10.0 |\n",
    "\n",
    "The sweep should **minimize** the metric `train_loss`.\n",
    "\n",
    "**Hints:**\n",
    "- Use `method: \"random\"` and specify each hyperparameter under `parameters` with a `values` list.\n",
    "- Instrument your training loop with `wandb.init(config=...)` and `wandb.log({\"train_loss\": ...})`.\n",
    "- Register your sweep with `sweep_id = wandb.sweep(sweep_config, project=\"chatbot-sweep\")`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define sweep_config using W&B random search over the hyperparameters listed above.\n",
    "# Then register the sweep:\n",
    "#   sweep_id = wandb.sweep(sweep_config, project='chatbot-sweep')\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrument the Training Loop for W&B\n",
    "\n",
    "Write a `train_sweep()` function that:\n",
    "1. Calls `wandb.init()` to start a new run\n",
    "2. Reads hyperparameter values from `wandb.config` (e.g. `wandb.config.learning_rate`)\n",
    "3. Builds the models and optimizers using those values\n",
    "4. Calls your training loop and logs `train_loss` at each `print_every` step with `wandb.log()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    \"\"\"Single sweep run invoked by the W&B agent.\n",
    "\n",
    "    Steps:\n",
    "      1. Call wandb.init() (use as a context manager).\n",
    "      2. Read hyperparameters from wandb.config\n",
    "         (learning_rate, optimizer, clip, teacher_forcing_ratio, decoder_learning_ratio).\n",
    "      3. Build fresh encoder/decoder and optimizers using those values.\n",
    "      4. Run the training loop; after every print_every iterations call\n",
    "         wandb.log({'train_loss': avg_loss, 'iteration': iteration}).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4 [5 points]: Run Hyperparameter Sweeps on GPU Colab\n",
    "\n",
    "Launch a W&B sweep agent on a **GPU-enabled** Colab runtime (Runtime > Change runtime type > T4 GPU). The agent will automatically sample configurations and execute `train_sweep()` for each one.\n",
    "\n",
    "- Run at least **10 sweep trials** to cover a meaningful portion of the search space.\n",
    "- Monitor results live in the [W&B console](https://wandb.ai).\n",
    "- Paste your W&B project URL or a screenshot of the sweep results dashboard below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Launch the W&B sweep agent.\n",
    "# Run at least 10 trials on a GPU-enabled Colab runtime.\n",
    "#   wandb.agent(sweep_id, function=train_sweep, count=10)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**W&B Project Link / Screenshot:**\n",
    "\n",
    "*Paste your W&B sweep URL here (e.g. https://wandb.ai/\\<username\\>/chatbot-sweep/sweeps/\\<sweep-id\\>) or embed a screenshot of the parallel coordinates / loss curves.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5 [10 points]: Analysis of Best Hyperparameters & Feature Importance\n",
    "\n",
    "After completing your sweeps, answer all four questions below.\n",
    "\n",
    "### 5a. Best Configuration\n",
    "Report the hyperparameter values that achieved the **lowest `train_loss`** across all sweep runs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Retrieve the best run programmatically via the W&B API:\n",
    "#\n",
    "# api = wandb.Api()\n",
    "# runs = api.runs(\"<your-entity>/chatbot-sweep\")\n",
    "# best_run = min(runs, key=lambda r: r.summary.get(\"train_loss\", float(\"inf\")))\n",
    "# print(\"Best config:\", best_run.config)\n",
    "# print(\"Best loss: \", best_run.summary[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best hyperparameter configuration:**\n",
    "\n",
    "| Hyperparameter | Best Value |\n",
    "|---|---|\n",
    "| `learning_rate` | *(your answer)* |\n",
    "| `optimizer` | *(your answer)* |\n",
    "| `clip` | *(your answer)* |\n",
    "| `teacher_forcing_ratio` | *(your answer)* |\n",
    "| `decoder_learning_ratio` | *(your answer)* |\n",
    "| **Best `train_loss`** | *(your answer)* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Feature Importance\n",
    "\n",
    "Use the **W&B feature importance panel** (available in the sweep UI under \"Parameter Importance\") to identify which hyperparameters had the greatest and least impact on `train_loss`.\n",
    "\n",
    "*Paste a screenshot of the feature importance chart here and list the top-3 most important hyperparameters.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Convergence Analysis\n",
    "\n",
    "Explain, in your own words, **why** the top hyperparameters from 5b affect model convergence. Address each of the following:\n",
    "\n",
    "- **Learning rate** — how does its magnitude affect gradient update steps and the risk of overshooting minima?\n",
    "- **Optimizer choice (Adam vs SGD)** — how do adaptive vs. fixed learning rates influence training on sparse/noisy sequence data?\n",
    "- **Gradient clipping (`clip`)** — why does clipping stabilize RNN training, and what happens when `clip=0` (no clipping) or `clip=100` (very loose)?\n",
    "- **Teacher forcing ratio** — how does the tradeoff between training with ground-truth vs. predicted tokens affect convergence speed and exposure bias?\n",
    "- **Decoder learning ratio** — why might the decoder benefit from a different learning rate than the encoder?\n",
    "\n",
    "*Write your analysis here (aim for 200–400 words).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Chatbot Quality\n",
    "\n",
    "Load the best checkpoint and interact with the chatbot. Report **at least 5 example exchanges** and briefly comment on the quality of the responses.\n",
    "\n",
    "```\n",
    "> <your input>\n",
    "Bot: <model output>\n",
    "\n",
    "> <your input>\n",
    "Bot: <model output>\n",
    "```\n",
    "\n",
    "*Replace the template above with your actual exchanges.*"
   ]
  }
 ]
}