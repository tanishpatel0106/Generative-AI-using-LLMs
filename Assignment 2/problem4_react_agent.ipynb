{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj9UG72nneAJ"
   },
   "source": [
    "# Question 4: Implement ReAct Agent with Multiple Tools (25 points)\n",
    "\n",
    "Implement a ReAct (Reasoning and Acting) agent as described by Yao et al. [1], incorporating three main tools: search, compare, and analyze. This agent should be able to handle complex queries by reasoning about which tool to use and when.\n",
    "\n",
    "a) (4 points) Implement the search tool using the SerpAPI integration from previous questions. Ensure it can be easily used by the ReAct agent.\n",
    "   - Proper integration with SerpAPI\n",
    "   - Formatting the search results for use by the ReAct agent\n",
    "\n",
    "b) (5 points) Create a custom comparison tool using LangChain's `Tool` class. The tool should accept multiple items and a category as input and return a comparison result.\n",
    "   - Implementing the comparison logic\n",
    "   - Creating an appropriate prompt template for the comparison\n",
    "   - Proper error handling for invalid inputs\n",
    "\n",
    "c) (5 points) Implement an analysis tool that can summarize and extract key information from search results or comparisons. This tool should use the OpenAI model to generate insightful analyses.\n",
    "   - Implementing the analysis logic\n",
    "   - Creating an appropriate prompt template for the analysis\n",
    "   - Ensuring the analysis output is concise and relevant\n",
    "\n",
    "d) (6 points) Integrate these tools with a ReAct agent using LangChain. Your implementation should:\n",
    "   - Use LangChain's `initialize_agent` function with the `AgentType.ZERO_SHOT_REACT_DESCRIPTION` agent type\n",
    "   - Include all three tools (search, compare, analyze) as available actions for the agent\n",
    "   - Implement proper error handling and fallback strategies\n",
    "   - Ensure smooth transitions between tools in the agent's reasoning process\n",
    "\n",
    "e) (5 points) Implement a simple Streamlit user interface for your ReAct agent. Your implementation should include:\n",
    "   - A text input field for users to enter their queries\n",
    "   - A button to submit the query and trigger the ReAct agent\n",
    "   - A display area for showing the final results\n",
    "   - A section to display the step-by-step reasoning process of the ReAct agent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYINe7f5o2ts",
    "outputId": "a663dc19-1767-4e6c-e334-ba4a1485019e"
   },
   "source": "# Install required packages\n# Pinned versions verified to work with this notebook\n!pip install \\\n    \"langchain==0.3.25\" \\\n    \"langchain-community==0.3.24\" \\\n    \"langchain-core==0.3.62\" \\\n    \"langchain-openai==0.2.14\" \\\n    \"google-search-results==2.4.2\" \\\n    \"streamlit>=1.32.0\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UoG8eGynneAL"
   },
   "source": "# Import necessary libraries\nimport os\nfrom google.colab import userdata\nfrom langchain_openai import OpenAI\nfrom langchain_core.tools import Tool                                          # langchain_core >= 0.1\nfrom langchain_community.agent_toolkits.load_tools import load_tools          # langchain_community >= 0.0.38\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\n# Set API keys\n# TODO: Retrieve your API keys from Colab secrets and set them as environment variables\nos.environ[\"OPENAI_API_KEY\"]  = userdata.get(\"OPENAI_API_KEY\")\nos.environ[\"SERPAPI_API_KEY\"] = userdata.get(\"SERPAPI_API_KEY\")\n\n# Initialize the OpenAI language model (Use temp=0)\n# TODO: Create an OpenAI instance with temperature=0\nllm = ...",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YsEDSkNneAM"
   },
   "source": [
    "## a) Implement the search tool"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hSqY-ZEkneAM"
   },
   "source": "# Load the search tool using SerpAPI\n# Hint: Use load_tools([\"serpapi\"], llm=llm) to get a LangChain-wrapped SerpAPI tool.\n# The returned list contains a ready-to-use Tool object â€” store it as `search_tool`.\n\n# TODO: Load the SerpAPI search tool\nsearch_tool = ...",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxZgkYH5neAN"
   },
   "source": [
    "## b) Create a custom comparison tool"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_GCQNo7nneAN"
   },
   "source": "def compare_items(query: str) -> str:\n    \"\"\"Compare multiple items in a given category.\n\n    The tool expects `query` to follow the format:\n        \"item1, item2, ..., category\"\n    For example: \"iPhone 15 Pro, Samsung Galaxy S24 Ultra, smartphones\"\n\n    Returns a string summarising the comparison, or an error message\n    if the input cannot be parsed.\n    \"\"\"\n    try:\n        # TODO: Split `query` on commas to extract individual tokens.\n        # The last token is the category; all preceding tokens are the items.\n        parts = ...\n        if len(parts) < 3:\n            return \"Error: please provide at least two items and a category.\"\n        items = ...       # all tokens except the last\n        category = ...    # the last token\n\n        # TODO: Create a PromptTemplate that asks the LLM to compare the items\n        # within the given category. Include {items} and {category} as variables.\n        comparison_template = \"\"\"...\"\"\"\n        comparison_prompt = PromptTemplate(\n            input_variables=[\"items\", \"category\"],\n            template=comparison_template\n        )\n\n        # TODO: Build an LLMChain and call .invoke() to run it.\n        # .invoke() takes a dict of template variables and returns a dict;\n        # access the generated text with [\"text\"].\n        comparison_chain = LLMChain(llm=llm, prompt=comparison_prompt)\n        result = comparison_chain.invoke({\"items\": ..., \"category\": ...})[\"text\"]\n        return result.strip()\n\n    except Exception as e:\n        return f\"Error in compare_items: {str(e)}\"\n\n\n# TODO: Wrap compare_items in a LangChain Tool object.\n# Give it a clear name and description so the ReAct agent knows when to use it.\ncompare_tool = Tool(\n    name=...,\n    func=compare_items,\n    description=...\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtxoHqPZneAN"
   },
   "source": [
    "## c) Implement an analysis tool"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FUJPXpXGneAN"
   },
   "source": "def analyze_results(query: str) -> str:\n    \"\"\"Summarize and extract key information from text passed by the agent.\n\n    Note: LangChain Tool functions must accept a *single* string argument.\n    The agent will pass whatever text it wants analyzed directly as `query`.\n\n    Returns a concise, insightful analysis string.\n    \"\"\"\n    # TODO: Create a PromptTemplate that instructs the LLM to produce a concise\n    # summary and highlight the key takeaways from the provided text.\n    # Use {text} as the only template variable.\n    analysis_template = \"\"\"...\"\"\"\n    analysis_prompt = PromptTemplate(\n        input_variables=[\"text\"],\n        template=analysis_template\n    )\n\n    # TODO: Build an LLMChain and call .invoke() to run it.\n    # .invoke() takes a dict of template variables and returns a dict;\n    # access the generated text with [\"text\"].\n    analysis_chain = LLMChain(llm=llm, prompt=analysis_prompt)\n    result = analysis_chain.invoke({\"text\": query})[\"text\"]\n    return result.strip()\n\n\n# TODO: Wrap analyze_results in a LangChain Tool object.\nanalyze_tool = Tool(\n    name=...,\n    func=analyze_results,\n    description=...\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV3D8WavneAN"
   },
   "source": [
    "## d) Integrate tools with a ReAct agent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9q1MuynIneAN"
   },
   "source": "# TODO: Build the list of tools for the ReAct agent.\n# Include search_tool, compare_tool, and analyze_tool.\ntools = [...]\n\n# TODO: Initialize the ReAct agent using initialize_agent().\n# Use AgentType.ZERO_SHOT_REACT_DESCRIPTION and set verbose=True so you can\n# observe the Thought / Action / Observation trace in the output.\nagent = initialize_agent(\n    tools=tools,\n    llm=llm,\n    agent=...,\n    verbose=...,\n    max_iterations=...,   # guard against infinite loops\n    handle_parsing_errors=True\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vkXwfEbxqaCA"
   },
   "source": "def process_query(query: str, max_steps: int = 100) -> str:\n    \"\"\"Run the ReAct agent on `query` and return its final answer.\n\n    Args:\n        query: The natural-language question to answer.\n        max_steps: Maximum number of reasoning steps before stopping.\n\n    Returns:\n        The agent's final answer as a string, or an error message.\n    \"\"\"\n    try:\n        # TODO: Call agent.invoke({\"input\": query}) and return the final answer.\n        # .invoke() returns a dict; access the answer with [\"output\"].\n        # The agent object was created in the cell above.\n        output = agent.invoke({\"input\": query})\n        return output[\"output\"]\n    except Exception as e:\n        return f\"Error processing query: {str(e)}\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## e) Streamlit User Interface\n\nCreate a separate Python file (e.g. `app.py`) containing a Streamlit app for your ReAct agent. Your app must include:\n\n- A **text input** field for the user's query\n- A **submit button** to trigger the agent\n- A **results area** that displays the final answer\n- *(Optional)* A collapsible section showing the step-by-step Thought / Action / Observation trace\n\n> **Note:** Streamlit apps must be run from the terminal with `streamlit run app.py`. You can use `%%writefile app.py` in a code cell to write the file directly from this notebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%writefile app.py\nimport os\nimport streamlit as st\nfrom langchain_openai import OpenAI\nfrom langchain_core.tools import Tool                                          # langchain_core >= 0.1\nfrom langchain_community.agent_toolkits.load_tools import load_tools          # langchain_community >= 0.0.38\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\n# TODO: Import and configure your LLM, tools, and agent here.\n# Use st.secrets[\"OPENAI_API_KEY\"] and st.secrets[\"SERPAPI_API_KEY\"]\n# to read keys when running outside Colab.\n\nst.title(\"ReAct Agent\")\nst.write(\"Ask a complex question and let the agent reason through it step by step.\")\n\n# TODO: Add a text input widget for the user's query.\nquery = st.text_input(...)\n\n# TODO: Add a button to submit the query.\nif st.button(...):\n    if query:\n        with st.spinner(\"Thinking...\"):\n            # TODO: Call process_query() (or agent.invoke()) and store the result.\n            result = ...\n\n        # TODO: Display the final answer.\n        st.subheader(\"Answer\")\n        st.write(result)\n\n        # (Optional) Display the step-by-step reasoning trace.\n        # Hint: use return_intermediate_steps=True on initialize_agent, then\n        # iterate over output[\"intermediate_steps\"] to show each action + observation.\n    else:\n        st.warning(\"Please enter a query before submitting.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "test-section-md",
   "metadata": {},
   "source": "## Test Your Implementation\n\nUse the cell below to test your implementation with a sample query."
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zr588QR3neAN"
   },
   "source": [
    "# Test your implementation\n",
    "sample_query = \"What are the top 3 smartphones in 2023, and how do they compare in terms of camera quality and battery life?\"\n",
    "\n",
    "result = process_query(sample_query)\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mEH_VAtneAN"
   },
   "source": "## Submission Requirements\n\nPlease submit the following items as part of your solution:\n\n1. Your complete code implementation for the ReAct agent and its tools (this notebook).\n2. Your `app.py` Streamlit file (Part e).\n3. A sample question that you used to test your tool (make it complex enough to demonstrate the use of multiple tools).\n4. The final answer provided by your ReAct agent for the sample question.\n5. The complete history traces of the ReAct agent for your sample question, showing its thought process, actions, and observations. Your traces should follow a format similar to this example:\n\n```\nThought: I need to find information about top smartphones first\nAction: Search[top smartphones 2023]\nObservation: [Search results about top smartphones]\nThought: Now I should compare the top two options\nAction: Compare[iPhone 14 Pro, Samsung Galaxy S23 Ultra, smartphones]\nObservation: [Comparison result]\nThought: I should analyze this comparison for the user\nAction: Analyze[comparison result]\nObservation: [Analysis of the comparison]\nFinal Answer: [Your agent's final response to the user's query]\n```\n\nEnsure that your submission clearly demonstrates the agent's ability to reason about which tool to use and how to interpret the results from each tool. Your history traces should show a logical flow of thoughts, actions, and observations, culminating in a final answer that addresses the initial query.\n\n**Note:** Ensure that your ReAct agent can seamlessly switch between these tools based on the task at hand. The agent should be able to reason about which tool to use next and how to interpret the results from each tool.\n\n## References\n\n[1] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). ReAct: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629. https://arxiv.org/pdf/2210.03629",
   "outputs": [],
   "execution_count": null
  }
 ]
}